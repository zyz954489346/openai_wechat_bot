import { remark } from 'remark'
import stripMarkdown from 'strip-markdown'
import { Configuration, OpenAIApi } from 'openai'
import {botName} from "../../config.js";
import dotenv from 'dotenv'
import { ChatOpenAI } from 'langchain/chat_models/openai';
import { HumanMessage, SystemMessage } from "langchain/schema";

const env = dotenv.config().parsed // ç¯å¢ƒå‚æ•°

const configuration = new Configuration({
  apiKey: env.OPENAI_API_KEY,
})
const openai = new OpenAIApi(configuration)

// GPT-3.5è°ƒç”¨
export async function getOpenAiReply(prompt) {
  console.log('ğŸš€ğŸš€ğŸš€ / prompt', prompt)
  //let chosen_model = 'text-davinci-003'
  let chosen_model = 'gpt-3.5-turbo'
  let reply = ''
  //'gpt-3.5-turbo',
  if (chosen_model === 'text-davinci-003'){
    console.log('ğŸš€ğŸš€ğŸš€ / Using model', chosen_model)
    const response = await openai.createCompletion({
        model: chosen_model,
        prompt: prompt,
        temperature: 0.4, // æ¯æ¬¡è¿”å›çš„ç­”æ¡ˆçš„ç›¸ä¼¼åº¦0-1ï¼ˆ0ï¼šæ¯æ¬¡éƒ½ä¸€æ ·ï¼Œ1ï¼šæ¯æ¬¡éƒ½ä¸ä¸€æ ·ï¼‰
        max_tokens: 4000,
        top_p: 1,
        frequency_penalty: 0.0,
        presence_penalty: 0.6,
        stop: [' Human:', ' AI:'],
      })

      reply = markdownToText(response.data.choices[0].text)
  } else if (chosen_model === 'gpt-3.5-turbo') {
    console.log('ğŸš€ğŸš€ğŸš€ / Using model', chosen_model)
    const response = await openai.createChatCompletion({
        model: chosen_model,
        messages:[
          {"role": "system", content:`ä½ æ˜¯ä¸€ä¸ªç¾¤èŠå°åŠ©æ‰‹ï¼Œä½ çš„åå­—å«'${botName}'ï¼Œè¯·ç”¨ä¸­æ–‡äº¤æµï¼Œå¹¶ä¸”ä¿æŒæ€åº¦äº²åˆ‡ï¼Œè¨€è¯­äº²åˆ‡é£è¶£å¹½é»˜ã€‚`},
          {"role": "user", content: prompt}
        ]})

      reply = markdownToText(response.data.choices[0].message.content)
  }
  console.log('ğŸš€ğŸš€ğŸš€ / reply', reply)
  return `${reply}\næ¥è‡ª ${chosen_model}`
}

// Dell-E ç»˜å›¾
export async function dallEImageReply(prompt) {
  console.log('ğŸš€ğŸš€ğŸš€ / Using model', 'DALL-E');
  let reply = '';
  const response = await openai.createImage({
    prompt,
    n: 1,
    size: '512x512'
  });

  reply = response.data.data[0].url;

  console.log('ğŸš€ğŸš€ğŸš€ / reply', reply)

  return reply;
}

// Markdonw è½¬æ–‡æœ¬
function markdownToText(markdown) {
  return remark()
    .use(stripMarkdown)
    .processSync(markdown ?? '')
    .toString()
}

// 2023-12-12 GPT4 ç¬¬ä¸‰æ–¹è°ƒç”¨
export const chatWithThirdOpenAI = async (prompt, model='gpt-4') => {

  const chat = new ChatOpenAI({
    configuration: {
      baseURL: 'https://api.gptapi.us/v1',
    },
    openAIApiKey: env.OPENAI_THIRD_API_KEY,
    temperature: 0.4,
    modelName: model,
    timeout: 120000
  })

  console.log('ğŸš€ğŸš€ğŸš€ / prompt', prompt)

  const response = await chat.call([
    new SystemMessage(`ä½ æ˜¯ä¸€ä¸ªç¾¤èŠå°åŠ©æ‰‹ï¼Œä½ çš„åå­—å«'${botName}'ï¼Œè¯·ç”¨ä¸­æ–‡äº¤æµï¼Œå¹¶ä¸”ä¿æŒæ€åº¦äº²åˆ‡ï¼Œè¨€è¯­äº²åˆ‡é£è¶£å¹½é»˜ã€‚`),
    new HumanMessage(prompt),
  ])

  console.log('ğŸš€ğŸš€ğŸš€ / reply', response)

  const reply = markdownToText(response.content)

  return `${reply}\næ¥è‡ª ${model}`
}


